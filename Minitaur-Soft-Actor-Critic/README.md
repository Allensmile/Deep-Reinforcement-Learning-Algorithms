# Project - MinitaurBulletEnv with Soft Actor Critic (SAC)

### Introduction

Solving the environment require an average total reward of over 15.0 over 100 consecutive episodes.      
We solve the MinitaurBulletEnv environment in __1745__ episodes, in __20__ hours, by usage of the __SAC__ algorithm,      
see the basic paper [SAC: Off-Policy Maximum Entropy Deep RL with a Stochastic Actor](https://arxiv.org/abs/1801.01290/).     

![](images/minitaur_2images.png)

### Training Score

![](images/plot_Minitaur-SAC_lr0.0001_b128_1745ep_sc=15.09.png)

### Steps of episodes

Here is a graph of the average number of steps for 100 series.

![](images/plot_Minitaur_AvgNumSteps_ep1745.png)

### Other SAC projects

* [AntBulletEnv](https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/Ant-PyBulletEnv-Soft-Actor-Critic)
* [BipedalWalker](https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalker-Soft-Actor-Critic)
* [HopperBulletEnv](https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/HopperBulletEnv-v0-SAC)
* [Walker2dBulletEnv](https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/Walker2DBulletEnv-v0_SAC)
